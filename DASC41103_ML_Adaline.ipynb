{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "e832d56c-6b1b-4188-99e3-5331b5115de7",
      "cell_type": "code",
      "source": "import sys\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff77b503-7dd0-486d-891f-e570973daa14",
      "cell_type": "code",
      "source": "# ## Training a perceptron model on the Iris dataset\n# ### Reading-in the Iris data\n\nfile_path = 'iris.data'\n\ntry:\n    df = pd.read_csv(file_path, header=None, encoding='utf-8')\n    print(f\"Successfully loaded '{file_path}'. First 5 rows:\")\n    print(df.head())\nexcept FileNotFoundError:\n    print(f\"Error: The file '{file_path}' was not found.\")\nexcept pd.errors.EmptyDataError:\n    print(f\"Error: The file '{file_path}' is empty or contains no data.\")\nexcept pd.errors.ParserError as e:\n    print(f\"Error: A parsing error occurred while reading '{file_path}': {e}\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a1add27c-8015-419e-8b5c-737a74f61b7b",
      "cell_type": "code",
      "source": "# process and standardize data\n# This is not an all inclusive function....you should improve!\ndef preprocess_data(df):\n\n    print(df.head())\n\n    # correct column names\n    new_column_names = ['sepal_l', 'sepal_w', 'petal_l', 'petal_w', 'class']\n    df.columns = new_column_names\n\n    # Handle missing values \n\n    # Binarize the target variable\n    df['class'] = df['class'].apply(lambda x: 1 if x == 'Iris-setosa' else 0)\n\n    # keep track of rows indexes to connect X and y\n    df = df.reset_index(names=['original_index'])\n\n    print(df.head())\n\n\n    # Encode categorical features\n\n    # change data types\n\n    # Separate features and target\n    X = df.drop(columns=['class'], axis=1)\n    y = df[['original_index','class']]\n\n    # Standardize numerical features\n    numeric_cols = ['sepal_l','sepal_w','petal_l','petal_w']\n    scaler = StandardScaler()\n    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n\n\n\n    return X, y",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "06fc4832-1b0d-484b-8666-80f394bce1ea",
      "cell_type": "code",
      "source": "# split data into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "afd45c75-573b-4826-9469-ea2da89dfe64",
      "cell_type": "code",
      "source": "class AdalineGD:\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    b_ : Scalar\n      Bias unit after fitting.\n    losses_ : list\n      Mean squared eror loss function values in each epoch.\n\n    \"\"\"\n    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the number of examples and\n          n_features is the number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n        self.b_ = np.float64(0.)\n        self.losses_ = []\n\n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            # Please note that the \"activation\" method has no effect\n            # in the code since it is simply an identity function. We\n            # could write `output = self.net_input(X)` directly instead.\n            # The purpose of the activation is more conceptual, i.e.,  \n            # in the case of logistic regression (as we will see later), \n            # we could change it to\n            # a sigmoid function to implement a logistic regression classifier.\n            output = self.activation(net_input)\n            errors = (y - output)          \n            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n            self.b_ += self.eta * 2.0 * errors.mean()\n            loss = (errors**2).mean()\n            self.losses_.append(loss)\n        return self\n\n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_\n\n    def activation(self, X):\n        \"\"\"Compute linear activation\"\"\"\n        return X\n\n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9586d0f7-7d17-4661-b05a-d85beb89c579",
      "cell_type": "code",
      "source": "# Train adaptive linear neuron with AdalineGD with 2 different learning rates\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\nada1 = AdalineGD(n_iter=15, eta=0.1).fit(X_train.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values, y_train.loc[:,'class'].values)\nax[0].plot(range(1, len(ada1.losses_) + 1), ada1.losses_, marker='o')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Mean squared error')\nax[0].set_title('Adaline - Learning rate 0.1')\n\nada2 = AdalineGD(n_iter=15, eta=0.0001).fit(X_train.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values, y_train.loc[:,'class'].values)\nax[1].plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Mean squared error')\nax[1].set_title('Adaline - Learning rate 0.0001')\n\n# Adjust the horizontal space between subplots\nplt.subplots_adjust(wspace=0.5)\n\n# plt.savefig('images/02_11.png', dpi=300)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c6f1fad3-303a-4a4e-8835-f2dc79bded2c",
      "cell_type": "code",
      "source": "# predict on trained AdalineGD models\ny_pred =ada1.predict(X_test.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values)\nprint(f\"Adaline accuracy with n_iter {n_temp} and eta {e_temp}: {accuracy_score(y_test.loc[:,'class'].values, y_pred):.4f}\")\n\ny_pred =ada2.predict(X_test.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values)\nprint(f\"Adaline accuracy with n_iter {n_temp} and eta {e_temp}: {accuracy_score(y_test.loc[:,'class'].values, y_pred):.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d2d3a3f0-0b58-4799-86e5-4de8ba91e2ae",
      "cell_type": "code",
      "source": "# looking at parameters eta and n_iter\nfig, ax = plt.subplots(nrows=5, ncols=5, figsize=(25, 25))\n\nlist_eta = [.1, 0.01, 0.001, 0.0001, 0.00001]\nlist_niter = [1, 5, 10, 15, 20]\n\nfig.suptitle(\"Created by Eric Specking\", fontsize=10, y=1)\n\nfor j, n_temp in enumerate(list_niter):\n\n    for i, e_temp in enumerate(list_eta):\n        ada = AdalineGD(n_iter=n_temp, eta= e_temp).fit(X_train.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values, y_train.loc[:,'class'].values)\n        ax[j, i].plot(range(1, len(ada.losses_) + 1), ada.losses_, marker='o')\n        ax[j, i].set_xlabel('Epochs', fontsize=10)\n        ax[j, i].set_ylabel('Mean squared error', fontsize=10)\n        ax[j, i].set_title(f\"eta: {e_temp}  niter: {n_temp}\", fontsize=10)\n\n        y_pred = ada.predict(X_test.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values)\n        print(f\"Adaline accuracy with n_iter {n_temp} and eta {e_temp}: {accuracy_score(y_test.loc[:,'class'].values, y_pred):.4f}\")\n    \nplt.tight_layout()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d822db51-d84b-4a4b-aa96-168711c8497b",
      "cell_type": "code",
      "source": "class AdalineSGD:\n    \"\"\"ADAptive LInear NEuron classifier.\n\n    Parameters\n    ------------\n    eta : float\n      Learning rate (between 0.0 and 1.0)\n    n_iter : int\n      Passes over the training dataset.\n    shuffle : bool (default: True)\n      Shuffles training data every epoch if True to prevent cycles.\n    random_state : int\n      Random number generator seed for random weight\n      initialization.\n\n\n    Attributes\n    -----------\n    w_ : 1d-array\n      Weights after fitting.\n    b_ : Scalar\n        Bias unit after fitting.\n    losses_ : list\n      Mean squared error loss function value averaged over all\n      training examples in each epoch.\n\n        \n    \"\"\"\n    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.w_initialized = False\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n    def fit(self, X, y):\n        \"\"\" Fit training data.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_examples, n_features]\n          Training vectors, where n_examples is the number of examples and\n          n_features is the number of features.\n        y : array-like, shape = [n_examples]\n          Target values.\n\n        Returns\n        -------\n        self : object\n\n        \"\"\"\n        self._initialize_weights(X.shape[1])\n        self.losses_ = []\n        for i in range(self.n_iter):\n            if self.shuffle:\n                X, y = self._shuffle(X, y)\n            losses = []\n            for xi, target in zip(X, y):\n                losses.append(self._update_weights(xi, target))\n            avg_loss = np.mean(losses)\n            self.losses_.append(avg_loss)\n        return self\n\n    def partial_fit(self, X, y):\n        \"\"\"Fit training data without reinitializing the weights\"\"\"\n        if not self.w_initialized:\n            self._initialize_weights(X.shape[1])\n        if y.ravel().shape[0] > 1:\n            for xi, target in zip(X, y):\n                self._update_weights(xi, target)\n        else:\n            self._update_weights(X, y)\n        return self\n\n    def _shuffle(self, X, y):\n        \"\"\"Shuffle training data\"\"\"\n        r = self.rgen.permutation(len(y))\n        return X[r], y[r]\n    \n    def _initialize_weights(self, m):\n        \"\"\"Initialize weights to small random numbers\"\"\"\n        self.rgen = np.random.RandomState(self.random_state)\n        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=m)\n        self.b_ = np.float64(0.)\n        self.w_initialized = True\n        \n    def _update_weights(self, xi, target):\n        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n        output = self.activation(self.net_input(xi))\n        error = (target - output)\n        self.w_ += self.eta * 2.0 * xi * (error)\n        self.b_ += self.eta * 2.0 * error\n        loss = error**2\n        return loss\n    \n    def net_input(self, X):\n        \"\"\"Calculate net input\"\"\"\n        return np.dot(X, self.w_) + self.b_\n\n    def activation(self, X):\n        \"\"\"Compute linear activation\"\"\"\n        return X\n\n    def predict(self, X):\n        \"\"\"Return class label after unit step\"\"\"\n        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5dded0d8-a3e4-439d-a3c5-3173ee91831d",
      "cell_type": "code",
      "source": "# Train adaptive linear neuron with AdalineSGD with 2 different learning rates.....without shuffling \nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\nada1 = AdalineSGD(n_iter=15, eta=0.1, shuffle=False).fit(X_train.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values, y_train.loc[:,'class'].values)\nax[0].plot(range(1, len(ada1.losses_) + 1), ada1.losses_, marker='o')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Mean squared error')\nax[0].set_title('Adaline - Learning rate 0.1')\n\nada2 = AdalineSGD(n_iter=15, eta=0.0001, shuffle=False).fit(X_train.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values, y_train.loc[:,'class'].values)\nax[1].plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Mean squared error')\nax[1].set_title('Adaline - Learning rate 0.0001')\n\n# Adjust the horizontal space between subplots\nplt.subplots_adjust(wspace=0.5)\n\n# plt.savefig('images/02_11.png', dpi=300)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "84b6267d-05af-4574-8f26-eed9946fda0d",
      "cell_type": "code",
      "source": "# predict on trained AdalineSGD models with shuffle FALSE\ny_pred =ada1.predict(X_test.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values)\nprint(f\"Adaline accuracy with n_iter 15 and eta 0.1: {accuracy_score(y_test.loc[:,'class'].values, y_pred):.4f}\")\n\ny_pred =ada2.predict(X_test.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values)\nprint(f\"Adaline accuracy with n_iter 15 and eta 0.0001: {accuracy_score(y_test.loc[:,'class'].values, y_pred):.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2dd4e0cc-9337-46a8-8c58-13daa244314d",
      "cell_type": "code",
      "source": "fig, ax = plt.subplots()\nada3 = AdalineSGD(n_iter=15, eta=0.0001, shuffle=True).fit(X_train.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values, y_train.loc[:,'class'].values)\nax.plot(range(1, len(ada1.losses_) + 1), ada1.losses_, marker='o')\nax.set_xlabel('Epochs')\nax.set_ylabel('Mean squared error')\nax.set_title('Adaline - Learning rate 0.0001')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9e957c10-c56c-4016-ad81-fb62a445fec4",
      "cell_type": "code",
      "source": "# predict on trained AdalineSGD models with shuffle true\ny_pred =ada3.predict(X_test.loc[:,['sepal_l','sepal_w','petal_l','petal_w']].values)\nprint(f\"Adaline accuracy with n_iter 15 and eta 0.0001: {accuracy_score(y_test.loc[:,'class'].values, y_pred):.4f}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}